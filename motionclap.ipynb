{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae449e2-c2af-400a-bdfa-df917392c29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665611bb-04ea-496f-8b45-0e07a6ac00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed3dac2f-e445-4137-a86f-71eae8375152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd66f6a-cbdd-49cb-b708-f8c740b915fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from modeling_clamp import ClampModel\n",
    "from configuration_clamp import ClampConfig\n",
    "\n",
    "from feature_extraction_clamp import ClampFeatureExtractor\n",
    "from transformers import RobertaTokenizer\n",
    "from processing_clamp import ClampProcessor\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "from clamp_dataset import load_dataset, simple_collate\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc76ab1e-3cd0-497b-a2df-bbff5903bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modeling_clap import ClapModel\n",
    "# from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449597b1-0f15-4539-86e1-ba19e68b6895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082a3fbd-c11a-49d3-9fb7-9ed8bb03ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllFile(base):\n",
    "    file_path = []\n",
    "    for root, ds, fs in os.walk(base, followlinks=True):\n",
    "        for f in fs:\n",
    "            fullname = os.path.join(root, f)\n",
    "            file_path.append(fullname)\n",
    "    return file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcaaedeb-29fb-4995-b203-598e7a90c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_config import cfg, get_cfg_defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151379a-553e-4cab-ad1c-53eac8e2c60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e7c22e6-9410-404f-a973-b7eb9a94bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config from: /srv/hays-lab/scratch/sanisetty3/music_motion/clamp/checkpoints/clamp_enc/clamp_enc.yaml\n"
     ]
    },
    {
     "ename": "ScannerError",
     "evalue": "while scanning for the next token\nfound character '`' that cannot start any token\n  in \"<unicode string>\", line 20, column 1:\n    `\n    ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScannerError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m cfg \u001b[38;5;241m=\u001b[39m get_cfg_defaults()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading config from:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yacs/config.py:212\u001b[0m, in \u001b[0;36mCfgNode.merge_from_file\u001b[0;34m(self, cfg_filename)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a yaml config file and merge it this CfgNode.\"\"\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cfg_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 212\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_from_other_cfg(cfg)\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yacs/config.py:363\u001b[0m, in \u001b[0;36mCfgNode.load_cfg\u001b[0;34m(cls, cfg_file_obj_or_str)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cfg_from_yaml_str(cfg_file_obj_or_str)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfg_file_obj_or_str, _FILE_TYPES):\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cfg_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_file_obj_or_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImpossible to reach here (unless there\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a bug)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yacs/config.py:372\u001b[0m, in \u001b[0;36mCfgNode._load_cfg_from_file\u001b[0;34m(cls, file_obj)\u001b[0m\n\u001b[1;32m    370\u001b[0m _, file_extension \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(file_obj\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_extension \u001b[38;5;129;01min\u001b[39;00m _YAML_EXTS:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_cfg_from_yaml_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_extension \u001b[38;5;129;01min\u001b[39;00m _PY_EXTS:\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_cfg_py_source(file_obj\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yacs/config.py:384\u001b[0m, in \u001b[0;36mCfgNode._load_cfg_from_yaml_str\u001b[0;34m(cls, str_obj)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_cfg_from_yaml_str\u001b[39m(\u001b[38;5;28mcls\u001b[39m, str_obj):\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a config from a YAML string encoding.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     cfg_as_dict \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(cfg_as_dict)\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/__init__.py:125\u001b[0m, in \u001b[0;36msafe_load\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_load\u001b[39m(stream):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/__init__.py:81\u001b[0m, in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m loader \u001b[38;5;241m=\u001b[39m Loader(stream)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     loader\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/constructor.py:49\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_document(node)\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/composer.py:36\u001b[0m, in \u001b[0;36mComposer.get_single_node\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(StreamEndEvent):\n\u001b[0;32m---> 36\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Ensure that the stream contains no more documents.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(StreamEndEvent):\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/composer.py:55\u001b[0m, in \u001b[0;36mComposer.compose_document\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_event()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Compose the root node.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Drop the DOCUMENT-END event.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_event()\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     82\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m---> 84\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mascend_resolver()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/composer.py:133\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    129\u001b[0m item_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m item_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[1;32m    135\u001b[0m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m     82\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[0;32m---> 84\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mascend_resolver()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/composer.py:127\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m anchor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors[anchor] \u001b[38;5;241m=\u001b[39m node\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMappingEndEvent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m#key_event = self.peek_event()\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     item_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m#if item_key in node.value:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/parser.py:98\u001b[0m, in \u001b[0;36mParser.check_event\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate:\n\u001b[0;32m---> 98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/parser.py:428\u001b[0m, in \u001b[0;36mParser.parse_block_mapping_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_block_mapping_key\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKeyToken\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    429\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_token(KeyToken, ValueToken, BlockEndToken):\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/scanner.py:116\u001b[0m, in \u001b[0;36mScanner.check_token\u001b[0;34m(self, *choices)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mchoices):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Check if the next token is one of the given types.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneed_more_tokens():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_more_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "File \u001b[0;32m/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/yaml/scanner.py:258\u001b[0m, in \u001b[0;36mScanner.fetch_more_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_plain()\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# No? It's an error. Let's produce a nice error message.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ScannerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile scanning for the next token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound character \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m that cannot start any token\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m ch,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mark())\n",
      "\u001b[0;31mScannerError\u001b[0m: while scanning for the next token\nfound character '`' that cannot start any token\n  in \"<unicode string>\", line 20, column 1:\n    `\n    ^"
     ]
    }
   ],
   "source": [
    "nme = \"clamp_enc\"\n",
    "path = f\"/srv/hays-lab/scratch/sanisetty3/music_motion/clamp/checkpoints/{nme}/{nme}.yaml\"\n",
    "cfg = get_cfg_defaults()\n",
    "print(\"loading config from:\", path)\n",
    "cfg.merge_from_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9fb6167-486c-4056-8df6-faabf1565ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp_config = ClampConfig.from_pretrained(\"/srv/hays-lab/scratch/sanisetty3/music_motion/clamp/clamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10103338-ae92-4442-bd7d-cc64d8bdc6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ClampModel were not initialized from the model checkpoint at /srv/hays-lab/scratch/sanisetty3/music_motion/clamp/clamp and are newly initialized: ['motion_projection.project_in.bias', 'motion_projection.project_in.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "clamp_model = ClampModel.from_pretrained(\"/srv/hays-lab/scratch/sanisetty3/music_motion/clamp/clamp\", use_safetensors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719767f6-09cd-42af-b38b-14f29db1df4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "446c987a-0ec2-4f24-8bcf-bbe5141dd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clamp_model.load_state_dict(\"/srv/hays-lab/scratch/sanisetty3/music_motion/clamp/clamp/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb73304-fce4-4617-842f-19644aeb6670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4886, -0.6965,  0.8278,  ...,  0.7765, -0.5237, -0.6707],\n",
       "        [-0.1029, -0.4814, -1.3239,  ...,  0.6291, -0.5265, -0.9828],\n",
       "        [ 0.2206, -0.1616, -0.2627,  ..., -0.2812,  0.8615, -1.7830],\n",
       "        ...,\n",
       "        [ 0.1758,  0.1818,  0.0020,  ..., -0.3373,  1.3992, -1.6280],\n",
       "        [-0.3789,  1.2745, -0.1851,  ..., -0.6624,  0.7807, -0.0992],\n",
       "        [-0.7875,  0.6957,  0.3103,  ...,  0.8619,  0.1014, -1.4184]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp_model.motion_model.quantizer.codebook.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb396d-dff3-427b-91a7-87fd5f6965e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7623b-439c-4e83-aeb8-fc06037b6413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea148ed9-7111-4712-bb76-e208e834536e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/hays-lab/flash5/sanisetty3/miniconda3/envs/tgm3d/lib/python3.9/site-packages/transformers/audio_utils.py:591: FutureWarning: The function `get_mel_filter_banks` is deprecated and will be removed in version 4.31.0 of Transformers\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clamp_feature_extractor = ClampFeatureExtractor.from_pretrained(\"/srv/hays-lab/scratch/sanisetty3/music_motion/clamp/clamp/\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"/srv/hays-lab/scratch/sanisetty3/music_motion/clamp/clamp\")\n",
    "clamp_processor = ClampProcessor(clamp_feature_extractor , tokenizer )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd5b4ca-8d8b-4c62-94da-6efcaed5f983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbd0d1-ca19-459d-840d-61fb303ca426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25582b0e-91cd-46bb-8b24-b62ea958b478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions animation: 268 and texts 268\n"
     ]
    }
   ],
   "source": [
    "dset , sampler, w = load_dataset(\"/srv/hays-lab/scratch/sanisetty3/motionx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee787b0d-03d7-47ec-994a-2307b1e14c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dset,\n",
    "        2,\n",
    "        sampler=sampler,\n",
    "        # shuffle=shuffle,\n",
    "        # num_workers=num_workers,\n",
    "        collate_fn=partial(simple_collate , clamp_processor = clamp_processor),\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40c4a027-7d07-4414-b961-bce838aa9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c64666d2-2b08-4509-ac5c-4f7d4fc55121",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = clamp_model(**batch , return_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e1bd4f2-ec74-4118-8009-c6fcd50d9598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_motion_features', 'motion_mask', 'input_ids', 'attention_mask', 'names', 'texts'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06a4c879-169d-480c-af0b-ca1fc36e557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_text_vs_motion = out.logits_per_text_vs_motion \n",
    "probs = logits_per_text_vs_motion.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df3ec86e-8620-4689-9d43-68dcc052f45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6821, 0.3179],\n",
       "        [0.3036, 0.6964]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58d61fad-3aad-406b-b7ec-9ffc66fc0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_motion_vs_text = out.logits_per_motion_vs_text\n",
    "probs2 = logits_per_motion_vs_text.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df941d07-5d91-4eae-9cd3-378e2168609c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7158, 0.6615], grad_fn=<DiagonalBackward0_copy>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9692c2f-8edd-4ab9-bc40-7704ed60bfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_motion_features', 'motion_mask', 'input_ids', 'attention_mask', 'names'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a3e73de7-12f1-43f9-86fa-82ec4d8e2df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['animation/subset_0001/Ways_To_Jump_Sit_Fall_Fist_Pump',\n",
       "       'animation/subset_0000/Ways_To_Catch_Coin_Toss'], dtype='<U53')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba1fe4d-5633-4473-b2da-6333f2cd2bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8eabfd4-256f-4529-895d-32c2602b2971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1024, 512)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp_model.motion_model.codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ec3e211c-93fb-4554-b203-f5b6e6752cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n =int( sum(batch[\"motion_mask\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "684793fc-6827-4549-8b28-d276a102f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_outputs = clamp_model.motion_model(\n",
    "                input_features=batch[\"input_motion_features\"],\n",
    "                motion_mask=batch[\"motion_mask\"],\n",
    "                return_dict=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb519607-ad7f-4d87-aa26-4eb8f6696bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 45, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_outputs.motion_quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c67e61-d7dc-47c2-9ad8-d34dba664c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dad411-82ee-4e7a-94df-4cd1ef3979f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfec79-71d4-4690-a5b6-f0b912381609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ceb865-3bf2-4d5e-9fcc-b30e786015cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4edbe-5441-4e25-b8d9-2004e79075c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725b42c-c485-44ab-9b0e-0d6aefa6e59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a06d6-b447-428f-b51b-b5816eaf68f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "266c20f5-689e-476a-b077-318eced6566d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3476, -0.3848, -0.3382,  ...,  1.2374,  1.4674, -2.1366],\n",
       "        [-0.6643, -0.1604, -1.1021,  ...,  0.5524, -1.2188, -2.4285],\n",
       "        [-0.4812, -0.7574, -1.0271,  ..., -0.4936,  1.6141, -2.4648],\n",
       "        ...,\n",
       "        [-0.7409, -0.1642, -0.5005,  ..., -0.7184,  0.2362, -0.9957],\n",
       "        [-1.1092,  0.1609, -1.3755,  ..., -0.2261,  1.0162, -2.4969],\n",
       "        [-1.0156,  0.1001, -1.6934,  ...,  0.4242,  1.1925, -2.5306]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_outputs.motion_embeds[0][: n//4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e2492e92-33e8-446c-80c0-ecccfd865eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7374,  0.1091, -0.9001,  ...,  0.9471,  0.4839, -1.7853],\n",
       "        [-0.6063, -0.2402, -1.3614,  ..., -1.4479, -0.4712, -2.0911],\n",
       "        [ 0.5954, -0.6793, -0.2143,  ..., -0.7939,  1.1101, -2.4060],\n",
       "        ...,\n",
       "        [ 0.4130, -1.0651,  0.4501,  ..., -0.1769,  0.1579, -1.4100],\n",
       "        [ 0.1780, -0.5427, -0.3067,  ..., -0.3478,  0.5527, -2.9065],\n",
       "        [-0.0870, -0.5428, -0.8138,  ..., -0.3544,  0.1502, -2.6137]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_outputs.motion_quantized[0][: n//4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887b8f5-d47c-45fb-837f-b4c45bf1b377",
   "metadata": {},
   "source": [
    "### OG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "75e2af18-7eca-45e0-b26f-db18f7a72e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4184, -0.4447, -0.3431,  ...,  1.2462,  1.4885, -2.1719],\n",
       "        [-0.4815, -0.2334, -1.2013,  ...,  0.5483, -1.2567, -2.4094],\n",
       "        [-0.3224, -0.8071, -1.0123,  ..., -0.5272,  1.6135, -2.1802],\n",
       "        ...,\n",
       "        [-0.4755, -0.4051, -0.1536,  ..., -1.0224,  0.1503, -1.9890],\n",
       "        [-1.7348,  0.5145, -1.3978,  ...,  0.0789,  0.8598, -2.9776],\n",
       "        [-0.4824, -0.1940, -1.5952,  ...,  0.2725,  0.6000, -2.4318]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_outputs.motion_embeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5a902cf9-c0b5-4799-94c3-216a06e37815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7374,  0.1091, -0.9001,  ...,  0.9471,  0.4839, -1.7853],\n",
       "        [-0.4930, -0.5167, -0.8416,  ..., -0.0847, -0.4262, -1.8722],\n",
       "        [ 0.5954, -0.6793, -0.2143,  ..., -0.7939,  1.1101, -2.4060],\n",
       "        ...,\n",
       "        [ 0.4130, -1.0651,  0.4501,  ..., -0.1769,  0.1579, -1.4100],\n",
       "        [ 0.0607, -0.4803, -0.3824,  ..., -0.3225,  0.4422, -2.5939],\n",
       "        [ 0.6482, -0.8899, -0.6191,  ...,  0.0393,  0.8039, -2.2377]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_outputs.motion_quantized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903b580-d7fd-4bf0-9a11-bf49646caa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d25b51-1a70-4424-96e3-33d42866d11e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m clamp_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mbatch\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43109201-b9dc-4d8d-81d3-c8e5d8dab63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528078f7-da42-4e4a-8d01-20307c23d1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4cf49-dd89-424e-991a-87640af81877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1783a7e2-0b3b-4aef-bcb6-7ce1c828386c",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "389c37fc-7326-4435-861f-620c36605216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_scale_a\n",
      "logit_scale_t\n",
      "logit_scale_m\n",
      "text_model.embeddings.word_embeddings.weight\n",
      "text_model.embeddings.position_embeddings.weight\n",
      "text_model.embeddings.token_type_embeddings.weight\n",
      "text_model.embeddings.LayerNorm.weight\n",
      "text_model.embeddings.LayerNorm.bias\n",
      "text_model.encoder.layer.0.attention.self.query.weight\n",
      "text_model.encoder.layer.0.attention.self.query.bias\n",
      "text_model.encoder.layer.0.attention.self.key.weight\n",
      "text_model.encoder.layer.0.attention.self.key.bias\n",
      "text_model.encoder.layer.0.attention.self.value.weight\n",
      "text_model.encoder.layer.0.attention.self.value.bias\n",
      "text_model.encoder.layer.0.attention.output.dense.weight\n",
      "text_model.encoder.layer.0.attention.output.dense.bias\n",
      "text_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.0.intermediate.dense.weight\n",
      "text_model.encoder.layer.0.intermediate.dense.bias\n",
      "text_model.encoder.layer.0.output.dense.weight\n",
      "text_model.encoder.layer.0.output.dense.bias\n",
      "text_model.encoder.layer.0.output.LayerNorm.weight\n",
      "text_model.encoder.layer.0.output.LayerNorm.bias\n",
      "text_model.encoder.layer.1.attention.self.query.weight\n",
      "text_model.encoder.layer.1.attention.self.query.bias\n",
      "text_model.encoder.layer.1.attention.self.key.weight\n",
      "text_model.encoder.layer.1.attention.self.key.bias\n",
      "text_model.encoder.layer.1.attention.self.value.weight\n",
      "text_model.encoder.layer.1.attention.self.value.bias\n",
      "text_model.encoder.layer.1.attention.output.dense.weight\n",
      "text_model.encoder.layer.1.attention.output.dense.bias\n",
      "text_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.1.intermediate.dense.weight\n",
      "text_model.encoder.layer.1.intermediate.dense.bias\n",
      "text_model.encoder.layer.1.output.dense.weight\n",
      "text_model.encoder.layer.1.output.dense.bias\n",
      "text_model.encoder.layer.1.output.LayerNorm.weight\n",
      "text_model.encoder.layer.1.output.LayerNorm.bias\n",
      "text_model.encoder.layer.2.attention.self.query.weight\n",
      "text_model.encoder.layer.2.attention.self.query.bias\n",
      "text_model.encoder.layer.2.attention.self.key.weight\n",
      "text_model.encoder.layer.2.attention.self.key.bias\n",
      "text_model.encoder.layer.2.attention.self.value.weight\n",
      "text_model.encoder.layer.2.attention.self.value.bias\n",
      "text_model.encoder.layer.2.attention.output.dense.weight\n",
      "text_model.encoder.layer.2.attention.output.dense.bias\n",
      "text_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.2.intermediate.dense.weight\n",
      "text_model.encoder.layer.2.intermediate.dense.bias\n",
      "text_model.encoder.layer.2.output.dense.weight\n",
      "text_model.encoder.layer.2.output.dense.bias\n",
      "text_model.encoder.layer.2.output.LayerNorm.weight\n",
      "text_model.encoder.layer.2.output.LayerNorm.bias\n",
      "text_model.encoder.layer.3.attention.self.query.weight\n",
      "text_model.encoder.layer.3.attention.self.query.bias\n",
      "text_model.encoder.layer.3.attention.self.key.weight\n",
      "text_model.encoder.layer.3.attention.self.key.bias\n",
      "text_model.encoder.layer.3.attention.self.value.weight\n",
      "text_model.encoder.layer.3.attention.self.value.bias\n",
      "text_model.encoder.layer.3.attention.output.dense.weight\n",
      "text_model.encoder.layer.3.attention.output.dense.bias\n",
      "text_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.3.intermediate.dense.weight\n",
      "text_model.encoder.layer.3.intermediate.dense.bias\n",
      "text_model.encoder.layer.3.output.dense.weight\n",
      "text_model.encoder.layer.3.output.dense.bias\n",
      "text_model.encoder.layer.3.output.LayerNorm.weight\n",
      "text_model.encoder.layer.3.output.LayerNorm.bias\n",
      "text_model.encoder.layer.4.attention.self.query.weight\n",
      "text_model.encoder.layer.4.attention.self.query.bias\n",
      "text_model.encoder.layer.4.attention.self.key.weight\n",
      "text_model.encoder.layer.4.attention.self.key.bias\n",
      "text_model.encoder.layer.4.attention.self.value.weight\n",
      "text_model.encoder.layer.4.attention.self.value.bias\n",
      "text_model.encoder.layer.4.attention.output.dense.weight\n",
      "text_model.encoder.layer.4.attention.output.dense.bias\n",
      "text_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.4.intermediate.dense.weight\n",
      "text_model.encoder.layer.4.intermediate.dense.bias\n",
      "text_model.encoder.layer.4.output.dense.weight\n",
      "text_model.encoder.layer.4.output.dense.bias\n",
      "text_model.encoder.layer.4.output.LayerNorm.weight\n",
      "text_model.encoder.layer.4.output.LayerNorm.bias\n",
      "text_model.encoder.layer.5.attention.self.query.weight\n",
      "text_model.encoder.layer.5.attention.self.query.bias\n",
      "text_model.encoder.layer.5.attention.self.key.weight\n",
      "text_model.encoder.layer.5.attention.self.key.bias\n",
      "text_model.encoder.layer.5.attention.self.value.weight\n",
      "text_model.encoder.layer.5.attention.self.value.bias\n",
      "text_model.encoder.layer.5.attention.output.dense.weight\n",
      "text_model.encoder.layer.5.attention.output.dense.bias\n",
      "text_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.5.intermediate.dense.weight\n",
      "text_model.encoder.layer.5.intermediate.dense.bias\n",
      "text_model.encoder.layer.5.output.dense.weight\n",
      "text_model.encoder.layer.5.output.dense.bias\n",
      "text_model.encoder.layer.5.output.LayerNorm.weight\n",
      "text_model.encoder.layer.5.output.LayerNorm.bias\n",
      "text_model.encoder.layer.6.attention.self.query.weight\n",
      "text_model.encoder.layer.6.attention.self.query.bias\n",
      "text_model.encoder.layer.6.attention.self.key.weight\n",
      "text_model.encoder.layer.6.attention.self.key.bias\n",
      "text_model.encoder.layer.6.attention.self.value.weight\n",
      "text_model.encoder.layer.6.attention.self.value.bias\n",
      "text_model.encoder.layer.6.attention.output.dense.weight\n",
      "text_model.encoder.layer.6.attention.output.dense.bias\n",
      "text_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.6.intermediate.dense.weight\n",
      "text_model.encoder.layer.6.intermediate.dense.bias\n",
      "text_model.encoder.layer.6.output.dense.weight\n",
      "text_model.encoder.layer.6.output.dense.bias\n",
      "text_model.encoder.layer.6.output.LayerNorm.weight\n",
      "text_model.encoder.layer.6.output.LayerNorm.bias\n",
      "text_model.encoder.layer.7.attention.self.query.weight\n",
      "text_model.encoder.layer.7.attention.self.query.bias\n",
      "text_model.encoder.layer.7.attention.self.key.weight\n",
      "text_model.encoder.layer.7.attention.self.key.bias\n",
      "text_model.encoder.layer.7.attention.self.value.weight\n",
      "text_model.encoder.layer.7.attention.self.value.bias\n",
      "text_model.encoder.layer.7.attention.output.dense.weight\n",
      "text_model.encoder.layer.7.attention.output.dense.bias\n",
      "text_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.7.intermediate.dense.weight\n",
      "text_model.encoder.layer.7.intermediate.dense.bias\n",
      "text_model.encoder.layer.7.output.dense.weight\n",
      "text_model.encoder.layer.7.output.dense.bias\n",
      "text_model.encoder.layer.7.output.LayerNorm.weight\n",
      "text_model.encoder.layer.7.output.LayerNorm.bias\n",
      "text_model.encoder.layer.8.attention.self.query.weight\n",
      "text_model.encoder.layer.8.attention.self.query.bias\n",
      "text_model.encoder.layer.8.attention.self.key.weight\n",
      "text_model.encoder.layer.8.attention.self.key.bias\n",
      "text_model.encoder.layer.8.attention.self.value.weight\n",
      "text_model.encoder.layer.8.attention.self.value.bias\n",
      "text_model.encoder.layer.8.attention.output.dense.weight\n",
      "text_model.encoder.layer.8.attention.output.dense.bias\n",
      "text_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.8.intermediate.dense.weight\n",
      "text_model.encoder.layer.8.intermediate.dense.bias\n",
      "text_model.encoder.layer.8.output.dense.weight\n",
      "text_model.encoder.layer.8.output.dense.bias\n",
      "text_model.encoder.layer.8.output.LayerNorm.weight\n",
      "text_model.encoder.layer.8.output.LayerNorm.bias\n",
      "text_model.encoder.layer.9.attention.self.query.weight\n",
      "text_model.encoder.layer.9.attention.self.query.bias\n",
      "text_model.encoder.layer.9.attention.self.key.weight\n",
      "text_model.encoder.layer.9.attention.self.key.bias\n",
      "text_model.encoder.layer.9.attention.self.value.weight\n",
      "text_model.encoder.layer.9.attention.self.value.bias\n",
      "text_model.encoder.layer.9.attention.output.dense.weight\n",
      "text_model.encoder.layer.9.attention.output.dense.bias\n",
      "text_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.9.intermediate.dense.weight\n",
      "text_model.encoder.layer.9.intermediate.dense.bias\n",
      "text_model.encoder.layer.9.output.dense.weight\n",
      "text_model.encoder.layer.9.output.dense.bias\n",
      "text_model.encoder.layer.9.output.LayerNorm.weight\n",
      "text_model.encoder.layer.9.output.LayerNorm.bias\n",
      "text_model.encoder.layer.10.attention.self.query.weight\n",
      "text_model.encoder.layer.10.attention.self.query.bias\n",
      "text_model.encoder.layer.10.attention.self.key.weight\n",
      "text_model.encoder.layer.10.attention.self.key.bias\n",
      "text_model.encoder.layer.10.attention.self.value.weight\n",
      "text_model.encoder.layer.10.attention.self.value.bias\n",
      "text_model.encoder.layer.10.attention.output.dense.weight\n",
      "text_model.encoder.layer.10.attention.output.dense.bias\n",
      "text_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.10.intermediate.dense.weight\n",
      "text_model.encoder.layer.10.intermediate.dense.bias\n",
      "text_model.encoder.layer.10.output.dense.weight\n",
      "text_model.encoder.layer.10.output.dense.bias\n",
      "text_model.encoder.layer.10.output.LayerNorm.weight\n",
      "text_model.encoder.layer.10.output.LayerNorm.bias\n",
      "text_model.encoder.layer.11.attention.self.query.weight\n",
      "text_model.encoder.layer.11.attention.self.query.bias\n",
      "text_model.encoder.layer.11.attention.self.key.weight\n",
      "text_model.encoder.layer.11.attention.self.key.bias\n",
      "text_model.encoder.layer.11.attention.self.value.weight\n",
      "text_model.encoder.layer.11.attention.self.value.bias\n",
      "text_model.encoder.layer.11.attention.output.dense.weight\n",
      "text_model.encoder.layer.11.attention.output.dense.bias\n",
      "text_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.11.intermediate.dense.weight\n",
      "text_model.encoder.layer.11.intermediate.dense.bias\n",
      "text_model.encoder.layer.11.output.dense.weight\n",
      "text_model.encoder.layer.11.output.dense.bias\n",
      "text_model.encoder.layer.11.output.LayerNorm.weight\n",
      "text_model.encoder.layer.11.output.LayerNorm.bias\n",
      "text_model.pooler.dense.weight\n",
      "text_model.pooler.dense.bias\n",
      "text_projection.linear1.weight\n",
      "text_projection.linear1.bias\n",
      "text_projection.linear2.weight\n",
      "text_projection.linear2.bias\n",
      "audio_model.audio_encoder.patch_embed.proj.weight\n",
      "audio_model.audio_encoder.patch_embed.proj.bias\n",
      "audio_model.audio_encoder.patch_embed.norm.weight\n",
      "audio_model.audio_encoder.patch_embed.norm.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.output.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.output.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.output.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.output.dense.bias\n",
      "audio_model.audio_encoder.layers.0.downsample.reduction.weight\n",
      "audio_model.audio_encoder.layers.0.downsample.norm.weight\n",
      "audio_model.audio_encoder.layers.0.downsample.norm.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.output.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.output.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.output.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.output.dense.bias\n",
      "audio_model.audio_encoder.layers.1.downsample.reduction.weight\n",
      "audio_model.audio_encoder.layers.1.downsample.norm.weight\n",
      "audio_model.audio_encoder.layers.1.downsample.norm.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.downsample.reduction.weight\n",
      "audio_model.audio_encoder.layers.2.downsample.norm.weight\n",
      "audio_model.audio_encoder.layers.2.downsample.norm.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.output.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.output.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.output.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.output.dense.bias\n",
      "audio_model.audio_encoder.batch_norm.weight\n",
      "audio_model.audio_encoder.batch_norm.bias\n",
      "audio_model.audio_encoder.norm.weight\n",
      "audio_model.audio_encoder.norm.bias\n",
      "audio_projection.linear1.weight\n",
      "audio_projection.linear1.bias\n",
      "audio_projection.linear2.weight\n",
      "audio_projection.linear2.bias\n",
      "motion_model.encoder.model.0.weight\n",
      "motion_model.encoder.model.0.bias\n",
      "motion_model.encoder.model.2.0.weight\n",
      "motion_model.encoder.model.2.0.bias\n",
      "motion_model.encoder.model.2.1.model.0.conv1.weight\n",
      "motion_model.encoder.model.2.1.model.0.conv1.bias\n",
      "motion_model.encoder.model.2.1.model.0.conv2.weight\n",
      "motion_model.encoder.model.2.1.model.0.conv2.bias\n",
      "motion_model.encoder.model.2.1.model.1.conv1.weight\n",
      "motion_model.encoder.model.2.1.model.1.conv1.bias\n",
      "motion_model.encoder.model.2.1.model.1.conv2.weight\n",
      "motion_model.encoder.model.2.1.model.1.conv2.bias\n",
      "motion_model.encoder.model.2.1.model.2.conv1.weight\n",
      "motion_model.encoder.model.2.1.model.2.conv1.bias\n",
      "motion_model.encoder.model.2.1.model.2.conv2.weight\n",
      "motion_model.encoder.model.2.1.model.2.conv2.bias\n",
      "motion_model.encoder.model.2.1.model.3.conv1.weight\n",
      "motion_model.encoder.model.2.1.model.3.conv1.bias\n",
      "motion_model.encoder.model.2.1.model.3.conv2.weight\n",
      "motion_model.encoder.model.2.1.model.3.conv2.bias\n",
      "motion_model.encoder.model.2.1.model.4.conv1.weight\n",
      "motion_model.encoder.model.2.1.model.4.conv1.bias\n",
      "motion_model.encoder.model.2.1.model.4.conv2.weight\n",
      "motion_model.encoder.model.2.1.model.4.conv2.bias\n",
      "motion_model.encoder.model.2.1.model.5.conv1.weight\n",
      "motion_model.encoder.model.2.1.model.5.conv1.bias\n",
      "motion_model.encoder.model.2.1.model.5.conv2.weight\n",
      "motion_model.encoder.model.2.1.model.5.conv2.bias\n",
      "motion_model.encoder.model.3.0.weight\n",
      "motion_model.encoder.model.3.0.bias\n",
      "motion_model.encoder.model.3.1.model.0.conv1.weight\n",
      "motion_model.encoder.model.3.1.model.0.conv1.bias\n",
      "motion_model.encoder.model.3.1.model.0.conv2.weight\n",
      "motion_model.encoder.model.3.1.model.0.conv2.bias\n",
      "motion_model.encoder.model.3.1.model.1.conv1.weight\n",
      "motion_model.encoder.model.3.1.model.1.conv1.bias\n",
      "motion_model.encoder.model.3.1.model.1.conv2.weight\n",
      "motion_model.encoder.model.3.1.model.1.conv2.bias\n",
      "motion_model.encoder.model.3.1.model.2.conv1.weight\n",
      "motion_model.encoder.model.3.1.model.2.conv1.bias\n",
      "motion_model.encoder.model.3.1.model.2.conv2.weight\n",
      "motion_model.encoder.model.3.1.model.2.conv2.bias\n",
      "motion_model.encoder.model.3.1.model.3.conv1.weight\n",
      "motion_model.encoder.model.3.1.model.3.conv1.bias\n",
      "motion_model.encoder.model.3.1.model.3.conv2.weight\n",
      "motion_model.encoder.model.3.1.model.3.conv2.bias\n",
      "motion_model.encoder.model.3.1.model.4.conv1.weight\n",
      "motion_model.encoder.model.3.1.model.4.conv1.bias\n",
      "motion_model.encoder.model.3.1.model.4.conv2.weight\n",
      "motion_model.encoder.model.3.1.model.4.conv2.bias\n",
      "motion_model.encoder.model.3.1.model.5.conv1.weight\n",
      "motion_model.encoder.model.3.1.model.5.conv1.bias\n",
      "motion_model.encoder.model.3.1.model.5.conv2.weight\n",
      "motion_model.encoder.model.3.1.model.5.conv2.bias\n",
      "motion_model.encoder.model.4.weight\n",
      "motion_model.encoder.model.4.bias\n",
      "motion_model.quantizer.codebook.weight\n",
      "motion_projection.linear1.weight\n",
      "motion_projection.linear1.bias\n",
      "motion_projection.linear2.weight\n",
      "motion_projection.linear2.bias\n"
     ]
    }
   ],
   "source": [
    "for n,p in clamp_model.named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f76b5-4395-4a3c-b3d0-070a1cdb8289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc86b6d-9d6d-4bb8-8ca2-6ed5171cb24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "333b667f-105e-4ad5-b781-b0de6abb8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_og = torch.load(\"/srv/hays-lab/scratch/sanisetty3/huggingface_downloads/models--laion--larger_clap_general/snapshots/ada0c23a36c4e8582805bb38fec3905903f18b41/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "95421f66-7810-4ea6-948d-00c7fca4a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_scale_a\n",
      "logit_scale_t\n",
      "text_model.embeddings.position_ids\n",
      "text_model.embeddings.token_type_ids\n",
      "text_model.embeddings.word_embeddings.weight\n",
      "text_model.embeddings.position_embeddings.weight\n",
      "text_model.embeddings.token_type_embeddings.weight\n",
      "text_model.embeddings.LayerNorm.weight\n",
      "text_model.embeddings.LayerNorm.bias\n",
      "text_model.encoder.layer.0.attention.self.query.weight\n",
      "text_model.encoder.layer.0.attention.self.query.bias\n",
      "text_model.encoder.layer.0.attention.self.key.weight\n",
      "text_model.encoder.layer.0.attention.self.key.bias\n",
      "text_model.encoder.layer.0.attention.self.value.weight\n",
      "text_model.encoder.layer.0.attention.self.value.bias\n",
      "text_model.encoder.layer.0.attention.output.dense.weight\n",
      "text_model.encoder.layer.0.attention.output.dense.bias\n",
      "text_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.0.intermediate.dense.weight\n",
      "text_model.encoder.layer.0.intermediate.dense.bias\n",
      "text_model.encoder.layer.0.output.dense.weight\n",
      "text_model.encoder.layer.0.output.dense.bias\n",
      "text_model.encoder.layer.0.output.LayerNorm.weight\n",
      "text_model.encoder.layer.0.output.LayerNorm.bias\n",
      "text_model.encoder.layer.1.attention.self.query.weight\n",
      "text_model.encoder.layer.1.attention.self.query.bias\n",
      "text_model.encoder.layer.1.attention.self.key.weight\n",
      "text_model.encoder.layer.1.attention.self.key.bias\n",
      "text_model.encoder.layer.1.attention.self.value.weight\n",
      "text_model.encoder.layer.1.attention.self.value.bias\n",
      "text_model.encoder.layer.1.attention.output.dense.weight\n",
      "text_model.encoder.layer.1.attention.output.dense.bias\n",
      "text_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.1.intermediate.dense.weight\n",
      "text_model.encoder.layer.1.intermediate.dense.bias\n",
      "text_model.encoder.layer.1.output.dense.weight\n",
      "text_model.encoder.layer.1.output.dense.bias\n",
      "text_model.encoder.layer.1.output.LayerNorm.weight\n",
      "text_model.encoder.layer.1.output.LayerNorm.bias\n",
      "text_model.encoder.layer.2.attention.self.query.weight\n",
      "text_model.encoder.layer.2.attention.self.query.bias\n",
      "text_model.encoder.layer.2.attention.self.key.weight\n",
      "text_model.encoder.layer.2.attention.self.key.bias\n",
      "text_model.encoder.layer.2.attention.self.value.weight\n",
      "text_model.encoder.layer.2.attention.self.value.bias\n",
      "text_model.encoder.layer.2.attention.output.dense.weight\n",
      "text_model.encoder.layer.2.attention.output.dense.bias\n",
      "text_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.2.intermediate.dense.weight\n",
      "text_model.encoder.layer.2.intermediate.dense.bias\n",
      "text_model.encoder.layer.2.output.dense.weight\n",
      "text_model.encoder.layer.2.output.dense.bias\n",
      "text_model.encoder.layer.2.output.LayerNorm.weight\n",
      "text_model.encoder.layer.2.output.LayerNorm.bias\n",
      "text_model.encoder.layer.3.attention.self.query.weight\n",
      "text_model.encoder.layer.3.attention.self.query.bias\n",
      "text_model.encoder.layer.3.attention.self.key.weight\n",
      "text_model.encoder.layer.3.attention.self.key.bias\n",
      "text_model.encoder.layer.3.attention.self.value.weight\n",
      "text_model.encoder.layer.3.attention.self.value.bias\n",
      "text_model.encoder.layer.3.attention.output.dense.weight\n",
      "text_model.encoder.layer.3.attention.output.dense.bias\n",
      "text_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.3.intermediate.dense.weight\n",
      "text_model.encoder.layer.3.intermediate.dense.bias\n",
      "text_model.encoder.layer.3.output.dense.weight\n",
      "text_model.encoder.layer.3.output.dense.bias\n",
      "text_model.encoder.layer.3.output.LayerNorm.weight\n",
      "text_model.encoder.layer.3.output.LayerNorm.bias\n",
      "text_model.encoder.layer.4.attention.self.query.weight\n",
      "text_model.encoder.layer.4.attention.self.query.bias\n",
      "text_model.encoder.layer.4.attention.self.key.weight\n",
      "text_model.encoder.layer.4.attention.self.key.bias\n",
      "text_model.encoder.layer.4.attention.self.value.weight\n",
      "text_model.encoder.layer.4.attention.self.value.bias\n",
      "text_model.encoder.layer.4.attention.output.dense.weight\n",
      "text_model.encoder.layer.4.attention.output.dense.bias\n",
      "text_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.4.intermediate.dense.weight\n",
      "text_model.encoder.layer.4.intermediate.dense.bias\n",
      "text_model.encoder.layer.4.output.dense.weight\n",
      "text_model.encoder.layer.4.output.dense.bias\n",
      "text_model.encoder.layer.4.output.LayerNorm.weight\n",
      "text_model.encoder.layer.4.output.LayerNorm.bias\n",
      "text_model.encoder.layer.5.attention.self.query.weight\n",
      "text_model.encoder.layer.5.attention.self.query.bias\n",
      "text_model.encoder.layer.5.attention.self.key.weight\n",
      "text_model.encoder.layer.5.attention.self.key.bias\n",
      "text_model.encoder.layer.5.attention.self.value.weight\n",
      "text_model.encoder.layer.5.attention.self.value.bias\n",
      "text_model.encoder.layer.5.attention.output.dense.weight\n",
      "text_model.encoder.layer.5.attention.output.dense.bias\n",
      "text_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.5.intermediate.dense.weight\n",
      "text_model.encoder.layer.5.intermediate.dense.bias\n",
      "text_model.encoder.layer.5.output.dense.weight\n",
      "text_model.encoder.layer.5.output.dense.bias\n",
      "text_model.encoder.layer.5.output.LayerNorm.weight\n",
      "text_model.encoder.layer.5.output.LayerNorm.bias\n",
      "text_model.encoder.layer.6.attention.self.query.weight\n",
      "text_model.encoder.layer.6.attention.self.query.bias\n",
      "text_model.encoder.layer.6.attention.self.key.weight\n",
      "text_model.encoder.layer.6.attention.self.key.bias\n",
      "text_model.encoder.layer.6.attention.self.value.weight\n",
      "text_model.encoder.layer.6.attention.self.value.bias\n",
      "text_model.encoder.layer.6.attention.output.dense.weight\n",
      "text_model.encoder.layer.6.attention.output.dense.bias\n",
      "text_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.6.intermediate.dense.weight\n",
      "text_model.encoder.layer.6.intermediate.dense.bias\n",
      "text_model.encoder.layer.6.output.dense.weight\n",
      "text_model.encoder.layer.6.output.dense.bias\n",
      "text_model.encoder.layer.6.output.LayerNorm.weight\n",
      "text_model.encoder.layer.6.output.LayerNorm.bias\n",
      "text_model.encoder.layer.7.attention.self.query.weight\n",
      "text_model.encoder.layer.7.attention.self.query.bias\n",
      "text_model.encoder.layer.7.attention.self.key.weight\n",
      "text_model.encoder.layer.7.attention.self.key.bias\n",
      "text_model.encoder.layer.7.attention.self.value.weight\n",
      "text_model.encoder.layer.7.attention.self.value.bias\n",
      "text_model.encoder.layer.7.attention.output.dense.weight\n",
      "text_model.encoder.layer.7.attention.output.dense.bias\n",
      "text_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.7.intermediate.dense.weight\n",
      "text_model.encoder.layer.7.intermediate.dense.bias\n",
      "text_model.encoder.layer.7.output.dense.weight\n",
      "text_model.encoder.layer.7.output.dense.bias\n",
      "text_model.encoder.layer.7.output.LayerNorm.weight\n",
      "text_model.encoder.layer.7.output.LayerNorm.bias\n",
      "text_model.encoder.layer.8.attention.self.query.weight\n",
      "text_model.encoder.layer.8.attention.self.query.bias\n",
      "text_model.encoder.layer.8.attention.self.key.weight\n",
      "text_model.encoder.layer.8.attention.self.key.bias\n",
      "text_model.encoder.layer.8.attention.self.value.weight\n",
      "text_model.encoder.layer.8.attention.self.value.bias\n",
      "text_model.encoder.layer.8.attention.output.dense.weight\n",
      "text_model.encoder.layer.8.attention.output.dense.bias\n",
      "text_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.8.intermediate.dense.weight\n",
      "text_model.encoder.layer.8.intermediate.dense.bias\n",
      "text_model.encoder.layer.8.output.dense.weight\n",
      "text_model.encoder.layer.8.output.dense.bias\n",
      "text_model.encoder.layer.8.output.LayerNorm.weight\n",
      "text_model.encoder.layer.8.output.LayerNorm.bias\n",
      "text_model.encoder.layer.9.attention.self.query.weight\n",
      "text_model.encoder.layer.9.attention.self.query.bias\n",
      "text_model.encoder.layer.9.attention.self.key.weight\n",
      "text_model.encoder.layer.9.attention.self.key.bias\n",
      "text_model.encoder.layer.9.attention.self.value.weight\n",
      "text_model.encoder.layer.9.attention.self.value.bias\n",
      "text_model.encoder.layer.9.attention.output.dense.weight\n",
      "text_model.encoder.layer.9.attention.output.dense.bias\n",
      "text_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.9.intermediate.dense.weight\n",
      "text_model.encoder.layer.9.intermediate.dense.bias\n",
      "text_model.encoder.layer.9.output.dense.weight\n",
      "text_model.encoder.layer.9.output.dense.bias\n",
      "text_model.encoder.layer.9.output.LayerNorm.weight\n",
      "text_model.encoder.layer.9.output.LayerNorm.bias\n",
      "text_model.encoder.layer.10.attention.self.query.weight\n",
      "text_model.encoder.layer.10.attention.self.query.bias\n",
      "text_model.encoder.layer.10.attention.self.key.weight\n",
      "text_model.encoder.layer.10.attention.self.key.bias\n",
      "text_model.encoder.layer.10.attention.self.value.weight\n",
      "text_model.encoder.layer.10.attention.self.value.bias\n",
      "text_model.encoder.layer.10.attention.output.dense.weight\n",
      "text_model.encoder.layer.10.attention.output.dense.bias\n",
      "text_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.10.intermediate.dense.weight\n",
      "text_model.encoder.layer.10.intermediate.dense.bias\n",
      "text_model.encoder.layer.10.output.dense.weight\n",
      "text_model.encoder.layer.10.output.dense.bias\n",
      "text_model.encoder.layer.10.output.LayerNorm.weight\n",
      "text_model.encoder.layer.10.output.LayerNorm.bias\n",
      "text_model.encoder.layer.11.attention.self.query.weight\n",
      "text_model.encoder.layer.11.attention.self.query.bias\n",
      "text_model.encoder.layer.11.attention.self.key.weight\n",
      "text_model.encoder.layer.11.attention.self.key.bias\n",
      "text_model.encoder.layer.11.attention.self.value.weight\n",
      "text_model.encoder.layer.11.attention.self.value.bias\n",
      "text_model.encoder.layer.11.attention.output.dense.weight\n",
      "text_model.encoder.layer.11.attention.output.dense.bias\n",
      "text_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "text_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "text_model.encoder.layer.11.intermediate.dense.weight\n",
      "text_model.encoder.layer.11.intermediate.dense.bias\n",
      "text_model.encoder.layer.11.output.dense.weight\n",
      "text_model.encoder.layer.11.output.dense.bias\n",
      "text_model.encoder.layer.11.output.LayerNorm.weight\n",
      "text_model.encoder.layer.11.output.LayerNorm.bias\n",
      "text_model.pooler.dense.weight\n",
      "text_model.pooler.dense.bias\n",
      "text_projection.linear1.weight\n",
      "text_projection.linear1.bias\n",
      "text_projection.linear2.weight\n",
      "text_projection.linear2.bias\n",
      "audio_model.audio_encoder.patch_embed.proj.weight\n",
      "audio_model.audio_encoder.patch_embed.proj.bias\n",
      "audio_model.audio_encoder.patch_embed.norm.weight\n",
      "audio_model.audio_encoder.patch_embed.norm.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.0.output.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.0.output.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.0.blocks.1.output.dense.weight\n",
      "audio_model.audio_encoder.layers.0.blocks.1.output.dense.bias\n",
      "audio_model.audio_encoder.layers.0.downsample.reduction.weight\n",
      "audio_model.audio_encoder.layers.0.downsample.norm.weight\n",
      "audio_model.audio_encoder.layers.0.downsample.norm.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.0.output.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.0.output.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.1.blocks.1.output.dense.weight\n",
      "audio_model.audio_encoder.layers.1.blocks.1.output.dense.bias\n",
      "audio_model.audio_encoder.layers.1.downsample.reduction.weight\n",
      "audio_model.audio_encoder.layers.1.downsample.norm.weight\n",
      "audio_model.audio_encoder.layers.1.downsample.norm.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.0.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.0.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.1.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.1.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.2.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.2.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.3.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.3.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.4.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.4.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.5.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.5.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.6.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.6.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.7.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.7.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.8.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.8.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.9.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.9.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.10.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.10.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.2.blocks.11.output.dense.weight\n",
      "audio_model.audio_encoder.layers.2.blocks.11.output.dense.bias\n",
      "audio_model.audio_encoder.layers.2.downsample.reduction.weight\n",
      "audio_model.audio_encoder.layers.2.downsample.norm.weight\n",
      "audio_model.audio_encoder.layers.2.downsample.norm.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.0.output.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.0.output.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.layernorm_before.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.layernorm_before.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.relative_position_bias_table\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.relative_position_index\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.query.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.query.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.key.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.key.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.value.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.self.value.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.output.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.attention.output.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.layernorm_after.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.layernorm_after.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.intermediate.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.intermediate.dense.bias\n",
      "audio_model.audio_encoder.layers.3.blocks.1.output.dense.weight\n",
      "audio_model.audio_encoder.layers.3.blocks.1.output.dense.bias\n",
      "audio_model.audio_encoder.batch_norm.weight\n",
      "audio_model.audio_encoder.batch_norm.bias\n",
      "audio_model.audio_encoder.batch_norm.running_mean\n",
      "audio_model.audio_encoder.batch_norm.running_var\n",
      "audio_model.audio_encoder.batch_norm.num_batches_tracked\n",
      "audio_model.audio_encoder.norm.weight\n",
      "audio_model.audio_encoder.norm.bias\n",
      "audio_projection.linear1.weight\n",
      "audio_projection.linear1.bias\n",
      "audio_projection.linear2.weight\n",
      "audio_projection.linear2.bias\n"
     ]
    }
   ],
   "source": [
    "for k,p in model_og.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "82f6af10-3640-4cdf-a7e0-a11cdc999804",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_motion = torch.load(\"/srv/hays-lab/scratch/sanisetty3/music_motion/ACMG/checkpoints/smplx_resnet/checkpoints/vqvae_motion.300000.pt\")[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4d0b5d7c-429d-4d2a-821b-9753d4b48f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7925c3d5-b0b3-4412-b04a-5547c470b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqvae.encoder.model.0.weight\n",
      "vqvae.encoder.model.0.bias\n",
      "vqvae.encoder.model.2.0.weight\n",
      "vqvae.encoder.model.2.0.bias\n",
      "vqvae.encoder.model.2.1.model.0.conv1.weight\n",
      "vqvae.encoder.model.2.1.model.0.conv1.bias\n",
      "vqvae.encoder.model.2.1.model.0.conv2.weight\n",
      "vqvae.encoder.model.2.1.model.0.conv2.bias\n",
      "vqvae.encoder.model.2.1.model.1.conv1.weight\n",
      "vqvae.encoder.model.2.1.model.1.conv1.bias\n",
      "vqvae.encoder.model.2.1.model.1.conv2.weight\n",
      "vqvae.encoder.model.2.1.model.1.conv2.bias\n",
      "vqvae.encoder.model.2.1.model.2.conv1.weight\n",
      "vqvae.encoder.model.2.1.model.2.conv1.bias\n",
      "vqvae.encoder.model.2.1.model.2.conv2.weight\n",
      "vqvae.encoder.model.2.1.model.2.conv2.bias\n",
      "vqvae.encoder.model.2.1.model.3.conv1.weight\n",
      "vqvae.encoder.model.2.1.model.3.conv1.bias\n",
      "vqvae.encoder.model.2.1.model.3.conv2.weight\n",
      "vqvae.encoder.model.2.1.model.3.conv2.bias\n",
      "vqvae.encoder.model.2.1.model.4.conv1.weight\n",
      "vqvae.encoder.model.2.1.model.4.conv1.bias\n",
      "vqvae.encoder.model.2.1.model.4.conv2.weight\n",
      "vqvae.encoder.model.2.1.model.4.conv2.bias\n",
      "vqvae.encoder.model.2.1.model.5.conv1.weight\n",
      "vqvae.encoder.model.2.1.model.5.conv1.bias\n",
      "vqvae.encoder.model.2.1.model.5.conv2.weight\n",
      "vqvae.encoder.model.2.1.model.5.conv2.bias\n",
      "vqvae.encoder.model.3.0.weight\n",
      "vqvae.encoder.model.3.0.bias\n",
      "vqvae.encoder.model.3.1.model.0.conv1.weight\n",
      "vqvae.encoder.model.3.1.model.0.conv1.bias\n",
      "vqvae.encoder.model.3.1.model.0.conv2.weight\n",
      "vqvae.encoder.model.3.1.model.0.conv2.bias\n",
      "vqvae.encoder.model.3.1.model.1.conv1.weight\n",
      "vqvae.encoder.model.3.1.model.1.conv1.bias\n",
      "vqvae.encoder.model.3.1.model.1.conv2.weight\n",
      "vqvae.encoder.model.3.1.model.1.conv2.bias\n",
      "vqvae.encoder.model.3.1.model.2.conv1.weight\n",
      "vqvae.encoder.model.3.1.model.2.conv1.bias\n",
      "vqvae.encoder.model.3.1.model.2.conv2.weight\n",
      "vqvae.encoder.model.3.1.model.2.conv2.bias\n",
      "vqvae.encoder.model.3.1.model.3.conv1.weight\n",
      "vqvae.encoder.model.3.1.model.3.conv1.bias\n",
      "vqvae.encoder.model.3.1.model.3.conv2.weight\n",
      "vqvae.encoder.model.3.1.model.3.conv2.bias\n",
      "vqvae.encoder.model.3.1.model.4.conv1.weight\n",
      "vqvae.encoder.model.3.1.model.4.conv1.bias\n",
      "vqvae.encoder.model.3.1.model.4.conv2.weight\n",
      "vqvae.encoder.model.3.1.model.4.conv2.bias\n",
      "vqvae.encoder.model.3.1.model.5.conv1.weight\n",
      "vqvae.encoder.model.3.1.model.5.conv1.bias\n",
      "vqvae.encoder.model.3.1.model.5.conv2.weight\n",
      "vqvae.encoder.model.3.1.model.5.conv2.bias\n",
      "vqvae.encoder.model.4.weight\n",
      "vqvae.encoder.model.4.bias\n",
      "vqvae.decoder.model.0.weight\n",
      "vqvae.decoder.model.0.bias\n",
      "vqvae.decoder.model.2.0.model.0.conv1.weight\n",
      "vqvae.decoder.model.2.0.model.0.conv1.bias\n",
      "vqvae.decoder.model.2.0.model.0.conv2.weight\n",
      "vqvae.decoder.model.2.0.model.0.conv2.bias\n",
      "vqvae.decoder.model.2.0.model.1.conv1.weight\n",
      "vqvae.decoder.model.2.0.model.1.conv1.bias\n",
      "vqvae.decoder.model.2.0.model.1.conv2.weight\n",
      "vqvae.decoder.model.2.0.model.1.conv2.bias\n",
      "vqvae.decoder.model.2.0.model.2.conv1.weight\n",
      "vqvae.decoder.model.2.0.model.2.conv1.bias\n",
      "vqvae.decoder.model.2.0.model.2.conv2.weight\n",
      "vqvae.decoder.model.2.0.model.2.conv2.bias\n",
      "vqvae.decoder.model.2.0.model.3.conv1.weight\n",
      "vqvae.decoder.model.2.0.model.3.conv1.bias\n",
      "vqvae.decoder.model.2.0.model.3.conv2.weight\n",
      "vqvae.decoder.model.2.0.model.3.conv2.bias\n",
      "vqvae.decoder.model.2.0.model.4.conv1.weight\n",
      "vqvae.decoder.model.2.0.model.4.conv1.bias\n",
      "vqvae.decoder.model.2.0.model.4.conv2.weight\n",
      "vqvae.decoder.model.2.0.model.4.conv2.bias\n",
      "vqvae.decoder.model.2.0.model.5.conv1.weight\n",
      "vqvae.decoder.model.2.0.model.5.conv1.bias\n",
      "vqvae.decoder.model.2.0.model.5.conv2.weight\n",
      "vqvae.decoder.model.2.0.model.5.conv2.bias\n",
      "vqvae.decoder.model.2.2.weight\n",
      "vqvae.decoder.model.2.2.bias\n",
      "vqvae.decoder.model.3.0.model.0.conv1.weight\n",
      "vqvae.decoder.model.3.0.model.0.conv1.bias\n",
      "vqvae.decoder.model.3.0.model.0.conv2.weight\n",
      "vqvae.decoder.model.3.0.model.0.conv2.bias\n",
      "vqvae.decoder.model.3.0.model.1.conv1.weight\n",
      "vqvae.decoder.model.3.0.model.1.conv1.bias\n",
      "vqvae.decoder.model.3.0.model.1.conv2.weight\n",
      "vqvae.decoder.model.3.0.model.1.conv2.bias\n",
      "vqvae.decoder.model.3.0.model.2.conv1.weight\n",
      "vqvae.decoder.model.3.0.model.2.conv1.bias\n",
      "vqvae.decoder.model.3.0.model.2.conv2.weight\n",
      "vqvae.decoder.model.3.0.model.2.conv2.bias\n",
      "vqvae.decoder.model.3.0.model.3.conv1.weight\n",
      "vqvae.decoder.model.3.0.model.3.conv1.bias\n",
      "vqvae.decoder.model.3.0.model.3.conv2.weight\n",
      "vqvae.decoder.model.3.0.model.3.conv2.bias\n",
      "vqvae.decoder.model.3.0.model.4.conv1.weight\n",
      "vqvae.decoder.model.3.0.model.4.conv1.bias\n",
      "vqvae.decoder.model.3.0.model.4.conv2.weight\n",
      "vqvae.decoder.model.3.0.model.4.conv2.bias\n",
      "vqvae.decoder.model.3.0.model.5.conv1.weight\n",
      "vqvae.decoder.model.3.0.model.5.conv1.bias\n",
      "vqvae.decoder.model.3.0.model.5.conv2.weight\n",
      "vqvae.decoder.model.3.0.model.5.conv2.bias\n",
      "vqvae.decoder.model.3.2.weight\n",
      "vqvae.decoder.model.3.2.bias\n",
      "vqvae.decoder.model.4.weight\n",
      "vqvae.decoder.model.4.bias\n",
      "vqvae.decoder.model.6.weight\n",
      "vqvae.decoder.model.6.bias\n",
      "vqvae.quantizer.codebook\n"
     ]
    }
   ],
   "source": [
    "for k,p in model_motion.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "222937bd-8d4a-4541-9005-1d3a4b5b228b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 263, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 4])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 4])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 3])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768, 1])\n",
      "torch.Size([768])\n",
      "torch.Size([512, 768, 3])\n",
      "torch.Size([512])\n",
      "tensor([[ 0.4886, -0.6965,  0.8278,  ...,  0.7765, -0.5237, -0.6707],\n",
      "        [-0.1029, -0.4814, -1.3239,  ...,  0.6291, -0.5265, -0.9828],\n",
      "        [ 0.2206, -0.1616, -0.2627,  ..., -0.2812,  0.8615, -1.7830],\n",
      "        ...,\n",
      "        [ 0.1758,  0.1818,  0.0020,  ..., -0.3373,  1.3992, -1.6280],\n",
      "        [-0.3789,  1.2745, -0.1851,  ..., -0.6624,  0.7807, -0.0992],\n",
      "        [-0.7875,  0.6957,  0.3103,  ...,  0.8619,  0.1014, -1.4184]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for k,p in model_motion.items():\n",
    "    if \"encoder\" in k:\n",
    "        print(p.shape)\n",
    "        new_dict[k.replace(\"vqvae\" , \"motion_model\")] = p\n",
    "    if \"codebook\" in k:\n",
    "        print(p)\n",
    "        new_dict[\"motion_model.quantizer.codebook.weight\"] = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2c9cc148-8408-41f0-80be-c16f308ff303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m , u = clamp_model.load_state_dict(model_og , strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "516ac916-efa0-4099-8bfe-c191f7498a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 , u2 = clamp_model.load_state_dict(new_dict, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bd38690c-ee02-4e80-9072-257b60b78804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4886, -0.6965,  0.8278,  ...,  0.7765, -0.5237, -0.6707],\n",
       "        [-0.1029, -0.4814, -1.3239,  ...,  0.6291, -0.5265, -0.9828],\n",
       "        [ 0.2206, -0.1616, -0.2627,  ..., -0.2812,  0.8615, -1.7830],\n",
       "        ...,\n",
       "        [ 0.1758,  0.1818,  0.0020,  ..., -0.3373,  1.3992, -1.6280],\n",
       "        [-0.3789,  1.2745, -0.1851,  ..., -0.6624,  0.7807, -0.0992],\n",
       "        [-0.7875,  0.6957,  0.3103,  ...,  0.8619,  0.1014, -1.4184]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp_model.motion_model.quantizer.codebook.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8b989-ec45-446d-a1e4-4a8b4b87b47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e606e-94ba-4af2-9bc9-b776e67bc070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea19221-82d7-45d7-851f-075eb639c9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab2cac-61e6-4415-93fb-319b6326acb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f6255-56e1-40a8-9af5-85f0fbec1b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae62c3f-078e-4a4f-a442-482a46b5d9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d4a3b-e9f9-41df-9663-e014d90b55ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3aed5cef-61b3-4612-b4e3-09b5947cc2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moyo mean 380.77647058823527 std 156.56916355329534 max 950 min 162\n",
      "music mean 244.91694725028057 std 112.2322020469831 max 943 min 1\n",
      "EgoBody mean 446.9142857142857 std 117.16863305265865 max 679 min 82\n",
      "fitness mean 213.24708485319618 std 127.85189517845544 max 1993 min 1\n",
      "beat mean 1706.0 std 0.0 max 1706 min 1706\n",
      "aist mean 426.40204211869815 std 893.2803941721178 max 10826 min 11\n",
      "dance mean 221.6851851851852 std 195.8577359806965 max 1496 min 5\n",
      "idea400 mean 206.37420076726343 std 65.0198867817423 max 569 min 1\n",
      "HAA500 mean 58.56643089275473 std 40.42577510598959 max 391 min 4\n",
      "kungfu mean 246.9153031761309 std 120.82275115239747 max 783 min 1\n",
      "perform mean 214.8357894736842 std 109.75524149170757 max 553 min 3\n",
      "GRAB mean 303.31760299625466 std 153.38501490937358 max 1114 min 122\n",
      "humanml mean 211.2161495962601 std 80.59329265648283 max 299 min 4\n",
      "choreomaster mean 4351.944444444444 std 1558.3454713331994 max 8142 min 886\n",
      "game_motion mean 108.68687561214496 std 102.12329450829242 max 2918 min 5\n",
      "animation mean 114.91489361702128 std 61.71962334492772 max 333 min 20\n",
      "humman mean 140.1034946236559 std 96.93571913232802 max 510 min 7\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(\"/srv/hays-lab/scratch/sanisetty3/motionx/motion_data/new_joint_vecs\"):\n",
    "    motions = []\n",
    "    for f in findAllFile(f\"/srv/hays-lab/scratch/sanisetty3/motionx/motion_data/new_joint_vecs/{i}\"):\n",
    "        motions.append(np.load(f).shape[0])\n",
    "\n",
    "    print(i , \"mean\" , np.mean(motions) , \"std\", np.std(motions) , \"max\" , max(motions) , \"min\" , min(motions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90961a3f-a53d-4013-b2fe-a99fd74c44d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a4de9-43db-4a29-8a92-0f92d8b7b33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e0a9f-e077-4cf7-b114-09f6fa9a723d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea379dd6-dcc5-498b-bf43-9ebac82284b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542aac1-388a-4bc5-b230-9b31d54ef418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
